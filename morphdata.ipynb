{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4627075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr\n",
    "sent_list = []\n",
    "for sent in parse_incr(open(\"../pstal-etu/sequoia/sequoia-ud.parseme.frsemcor.simple.small\", encoding='UTF-8')):\n",
    "    sent_list.append(sent.metadata['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b782c252-b934-4dc2-98a9-1a8f279809c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "6\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "chaine = \"La gare routiere attend\"\n",
    "for idx, w in enumerate(chaine): \n",
    "    if w == ' ':\n",
    "        print(idx-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f5cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sent_list)\n",
    "# data = {\n",
    "#     chars\n",
    "# }\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "chars = []\n",
    "in_enc = []\n",
    "ends = []\n",
    "vocab = [\"<pad>\", \"<unk>\"] + sorted(set(\"\".join(sent_list)))    \n",
    "vocab_int = {\"<pad>\": 0, \"<unk>\": 1, \"<esp>\": 2}\n",
    "for i, v in enumerate(vocab):\n",
    "    if v == ' ':\n",
    "        vocab[i] = '<esp>'\n",
    "        \n",
    "for s in sent_list[:1]:\n",
    "    c = [\"<pad>\"] + [w if w != \" \" else \"<esp>\" for w in s]\n",
    "    chars.append(c)\n",
    "\n",
    "    char_to_int = [vocab.index(w) for w in c]\n",
    "    in_enc.append(char_to_int)\n",
    "    \n",
    "    end = []\n",
    "    for idx, w in enumerate(s): \n",
    "        if w == ' ':\n",
    "          end.append(idx-1)     \n",
    "    ends.append(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32109dbf-23c0-4b85-adaa-0ddc280b9fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', 'L', \"'\", 'a', 's', 's', 'o', 'c', 'i', 'a', 't', 'i', 'o', 'n', '<esp>', 'a', '<esp>', 'c', 'h', 'a', 'n', 'g', 'é', '<esp>', 'l', 'e', 's', '<esp>', 'd', 'é', 'c', 'o', 'r', 's', '<esp>', 'e', 't', '<esp>', 'a', 'v', 'e', 'c', '<esp>', 'l', \"'\", 'a', 'i', 'd', 'e', '<esp>', 'd', 'e', '<esp>', 'p', 'l', 'u', 's', 'i', 'e', 'u', 'r', 's', '<esp>', 'b', 'é', 'n', 'é', 'v', 'o', 'l', 'e', 's', ',', '<esp>', 'é', 't', 'a', 'b', 'l', 'i', '<esp>', 'd', 'i', 'f', 'f', 'é', 'r', 'e', 'n', 't', 's', '<esp>', 't', 'a', 'b', 'l', 'e', 'a', 'u', 'x', '<esp>', 's', 'u', 'r', '<esp>', 'l', 'e', '<esp>', 't', 'h', 'è', 'm', 'e', '<esp>', 'd', 'e', '<esp>', '\"', 'C', 'i', 'n', 'q', '<esp>', 's', 'i', 'è', 'c', 'l', 'e', 's', '<esp>', 'd', \"'\", 'a', 'c', 't', 'i', 'v', 'i', 't', 'é', '<esp>', 'é', 'c', 'o', 'n', 'o', 'm', 'i', 'q', 'u', 'e', '<esp>', 'd', 'e', '<esp>', 'l', 'a', '<esp>', 'r', 'é', 'g', 'i', 'o', 'n', '<esp>', 'd', \"'\", 'A', 'n', 'c', 'e', 'r', 'v', 'i', 'l', 'l', 'e', '\"', '.']\n",
      "\n",
      "\n",
      "[0, 30, 5, 39, 56, 56, 52, 41, 47, 39, 57, 47, 52, 51, 2, 39, 2, 41, 46, 39, 51, 45, 68, 2, 49, 43, 56, 2, 42, 68, 41, 52, 55, 56, 2, 43, 57, 2, 39, 59, 43, 41, 2, 49, 5, 39, 47, 42, 43, 2, 42, 43, 2, 53, 49, 58, 56, 47, 43, 58, 55, 56, 2, 40, 68, 51, 68, 59, 52, 49, 43, 56, 6, 2, 68, 57, 39, 40, 49, 47, 2, 42, 47, 44, 44, 68, 55, 43, 51, 57, 56, 2, 57, 39, 40, 49, 43, 39, 58, 60, 2, 56, 58, 55, 2, 49, 43, 2, 57, 46, 67, 50, 43, 2, 42, 43, 2, 4, 22, 47, 51, 54, 2, 56, 47, 67, 41, 49, 43, 56, 2, 42, 5, 39, 41, 57, 47, 59, 47, 57, 68, 2, 68, 41, 52, 51, 52, 50, 47, 54, 58, 43, 2, 42, 43, 2, 49, 39, 2, 55, 68, 45, 47, 52, 51, 2, 42, 5, 20, 51, 41, 43, 55, 59, 47, 49, 49, 43, 4, 8]\n",
      "\n",
      "\n",
      "[12, 14, 21, 25, 32, 35, 40, 47, 50, 60, 71, 78, 89, 98, 102, 105, 111, 114, 120, 128, 139, 150, 153, 156, 163]\n"
     ]
    }
   ],
   "source": [
    "print(chars[0])\n",
    "print(\"\\n\")\n",
    "print(in_enc[0])\n",
    "print(\"\\n\")\n",
    "print(ends[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b8923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from use_conllulib import CoNLLUReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd60370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"../pstal-etu/sequoia/sequoia-ud.parseme.frsemcor.simple.small\", encoding='UTF-8')\n",
    "conllu_reader = CoNLLUReader(infile=infile) \n",
    "col_name_dict = {\n",
    "    \"form\": [],  # tokens (words)\n",
    "    \"upos\": []            # POS tags\n",
    "}\n",
    "_, vocab = conllu_reader.to_int_and_vocab(col_name_dict)\n",
    "wordvocab = vocab['form']\n",
    "tagvocab = vocab['upos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24df0656",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = vocab['upos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e16bae01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gender': {'Gender', 'Number'}, 'Number': {'Gender', 'Number'}, 'PronType': {'PronType', 'Definite', 'Number'}, 'Person': {'Mood', 'VerbForm', 'Tense', 'Number', 'Person'}, 'Mood': {'Mood', 'VerbForm', 'Tense', 'Number', 'Person'}, 'Tense': {'VerbForm', 'Voice', 'Gender', 'Tense', 'Number'}, 'VerbForm': {'VerbForm'}, 'Definite': {'PronType', 'Definite', 'Number'}, 'NumType': {'NumType', 'Number'}, 'Voice': {'VerbForm', 'Voice', 'Gender', 'Tense', 'Number'}, 'Poss': {'Poss', 'Gender', 'Number'}, 'Polarity': {'Polarity'}, 'Reflex': {'Reflex', 'Person'}, 'Foreign': {'Foreign'}}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import conllu\n",
    "import sys\n",
    "\n",
    "file = open(\"../pstal-etu/sequoia/sequoia-ud.parseme.frsemcor.simple.full\", encoding='UTF-8')\n",
    "\n",
    "featuresless_counter = 0\n",
    "features_names = {}\n",
    "\n",
    "\n",
    "for sent in conllu.parse_incr(file):\n",
    "    for word in sent:\n",
    "        if word['feats'] is None:\n",
    "            featuresless_counter += 1\n",
    "        else:\n",
    "            for key in word[\"feats\"]:\n",
    "                features_names[key] = set(word['feats'])\n",
    "\n",
    "print(features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc10bde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenList<L', association, a, changé, les, décors, et, avec, l', aide, de, plusieurs, bénévoles, ,, établi, différents, tableaux, sur, le, thème, de, \", Cinq, siècles, d', activité, économique, de, la, région, d', Ancerville, \", ., metadata={global.columns: \"ID FORM LEMMA UPOS XPOS FEATS HEAD DEPREL DEPS MISC PARSEME:MWE FRSEMCOR:NOUN PARSEME:NE\", sent_id: \"annodis.er_00014\", text: \"L'association a changé les décors et avec l'aide de plusieurs bénévoles, établi différents tableaux sur le thème de \"Cinq siècles d'activité économique de la région d'Ancerville\".\"}>\n",
      "TokenList<Ouverture, tous, les, jours, sauf, le, lundi, de, 14, h, 30, à, 18, h, ., metadata={sent_id: \"annodis.er_00022\", text: \"Ouverture tous les jours sauf le lundi de 14h30 à 18h.\"}>\n",
      "TokenList<Quant, à, le, sous-préfet, ,, il, apprécie, l', énergie, dépensée, pour, une, telle, réalisation, ., metadata={sent_id: \"annodis.er_00037\", text: \"Quant au sous-préfet, il apprécie l'énergie dépensée pour une telle réalisation.\"}>\n",
      "TokenList<Les, membres, de, le, club, auront, l', occasion, de, s', entraîner, cet, après-midi, ,, dès, 14, h, ,, salle, Jean-Mathieu, ., metadata={sent_id: \"annodis.er_00063\", text: \"Les membres du club auront l'occasion de s'entraîner cet après-midi, dès 14h, salle Jean-Mathieu.\"}>\n",
      "TokenList<M., Hosneld, avait, 44, ans, ., metadata={sent_id: \"annodis.er_00070\", text: \"M. Hosneld avait 44 ans.\"}>\n",
      "TokenList<C', est, le, cas, de, ce, brave, Joseph, Bari, ,, toujours, présent, à, le, comité, ,, vedette, de, la, colombophilie, jusqu', en, 1962, ,, époque, à, laquelle, il, fut, obligé, de, quitter, son, pigeonnier, de, Châtenois-les-Forges, pour, aller, habiter, un, espace, réduit, à, Grand-Charmont, ., metadata={sent_id: \"annodis.er_00090\", text: \"C'est le cas de ce brave Joseph Bari, toujours présent au comité, vedette de la colombophilie jusqu'en 1962, époque à laquelle il fut obligé de quitter son pigeonnier de Châtenois-les-Forges pour aller habiter un espace réduit à Grand-Charmont.\"}>\n",
      "TokenList<Pour, animer, cette, soirée, ,, François, Puel, ,, chercheur, de, le, laboratoire, d', astrophysique, de, Besançon, ., metadata={sent_id: \"annodis.er_00128\", text: \"Pour animer cette soirée, François Puel, chercheur du laboratoire d'astrophysique de Besançon.\"}>\n",
      "TokenList<Les, Petits, Princes, ,, à, le, nombre, de, cinq, ,, le, roi, Pierre, ..., ont, réveillé, chez, le, public, des, souvenirs, d', enfance, ,, le, temps, d', une, représentation, ., metadata={sent_id: \"annodis.er_00143\", text: \"Les Petits Princes, au nombre de cinq, le roi Pierre... ont réveillé chez le public des souvenirs d'enfance, le temps d'une représentation.\"}>\n",
      "TokenList<Le, double, événement, était, fêté, comme, il, se, doit, et, conjointement, par, les, deux, établissements, ,, mercredi, 23, juin, ,, à, l', occasion, de, l', assemblée, générale, décentralisée, de, les, Offices, de, tourisme, et, Syndicats, d', initiative, de, le, Doubs, qui, se, tenait, le, même, jour, ,, sous, la, présidence, d', Edmond, Maire, ,, à, la, Saline, Royale, ., metadata={sent_id: \"annodis.er_00154\", text: \"Le double événement était fêté comme il se doit et conjointement par les deux établissements, mercredi 23 juin, à l'occasion de l'assemblée générale décentralisée des Offices de tourisme et Syndicats d'initiative du Doubs qui se tenait le même jour, sous la présidence d'Edmond Maire, à la Saline Royale.\"}>\n",
      "TokenList<Suzanne, Collin, était, née, à, Brauvilliers, dans, la, Meuse, ,, le, 21, janvier, 1924, ,, issue, d', une, famille, de, cinq, enfants, ., metadata={sent_id: \"annodis.er_00163\", text: \"Suzanne Collin était née à Brauvilliers dans la Meuse, le 21 janvier 1924, issue d'une famille de cinq enfants.\"}>\n",
      "TokenList<Quand, elle, ne, vaquait, pas, à, ses, occupations, ménagères, ,, elle, utilisait, les, transports, en, commun, et, descendait, seule, faire, ses, courses, en, centre, ville, ,, et, prenait, alors, plaisir, à, faire, un, brin, de, causette, avec, ses, copines, de, rencontre, ., metadata={sent_id: \"annodis.er_00170\", text: \"Quand elle ne vaquait pas à ses occupations ménagères, elle utilisait les transports en commun et descendait seule faire ses courses en centre ville, et prenait alors plaisir à faire un brin de causette avec ses copines de rencontre.\"}>\n",
      "TokenList<Le, fond, de, le, problème, :, l', opposition, avec, le, président, de, l', Harmonie, ,, accusé, de, \", prendre, seul, les, décisions, \", ., metadata={sent_id: \"annodis.er_00182\", text: \"Le fond du problème : l'opposition avec le président de l'Harmonie, accusé de \"prendre seul les décisions\".\"}>\n",
      "TokenList<Selon, les, démissionnaires, ,, il, ne, reste, plus, aujourd'hui, que, trois, clairons, et, un, tambour, à, le, sein, de, la, batterie, ., metadata={sent_id: \"annodis.er_00186\", text: \"Selon les démissionnaires, il ne reste plus aujourd'hui que trois clairons et un tambour au sein de la batterie.\"}>\n",
      "TokenList<\", Verdun, ,, ville, de, lumière, !, \", metadata={sent_id: \"annodis.er_00190\", text: \"\"Verdun, ville de lumière !\"\"}>\n",
      "TokenList<La, gare, routière, attend, toujours, ses, illuminations, ,, pas, des, guirlandes, ,, mais, des, lampadaires, pour, que, scolaires, ,, usagés, et, conducteurs, se, sentent, en, sécurité, ., metadata={sent_id: \"annodis.er_00192\", text: \"La gare routière attend toujours ses illuminations, pas des guirlandes, mais des lampadaires pour que scolaires, usagés et conducteurs se sentent en sécurité.\"}>\n",
      "TokenList<Mais, dans, le, secteur, de, le, Pré, L', Evêque, et, les, prés, avoisinants, ,, la, glace, est, encore, bien, présente, et, risque, d', être, encore, là, pendant, plusieurs, jours, ., metadata={sent_id: \"annodis.er_00203\", text: \"Mais dans le secteur du Pré L'Evêque et les prés avoisinants, la glace est encore bien présente et risque d'être encore là pendant plusieurs jours.\"}>\n",
      "TokenList<\", Ce, ne, sera, pas, un, cours, magistral, ,, mais, plutôt, un, partage, de, connaissances, \", ,, souligne, Dominique, Richard, ,, responsable, de, les, animations, dans, le, village, ., metadata={sent_id: \"annodis.er_00217\", text: \"\"Ce ne sera pas un cours magistral, mais plutôt un partage de connaissances\", souligne Dominique Richard, responsable des animations dans le village.\"}>\n",
      "TokenList<L', EBM, s', inclina, devant, Joeuf, par, 70-61, ., metadata={sent_id: \"annodis.er_00228\", text: \"L'EBM s'inclina devant Joeuf par 70-61.\"}>\n",
      "TokenList<Le, conducteur, présente, des, signes, d', ivresse, et, est, conduit, à, le, commissariat, central, où, son, taux, d', alcoolémie, est, fixé, à, 1,74, gramme, par, litre, de, sang, ., metadata={sent_id: \"annodis.er_00238\", text: \"Le conducteur présente des signes d'ivresse et est conduit au commissariat central où son taux d'alcoolémie est fixé à 1,74 gramme par litre de sang.\"}>\n",
      "TokenList<L', association, paroissiale, investit, dans, les, locaux, de, la, cure, ., metadata={sent_id: \"annodis.er_00242\", text: \"L'association paroissiale investit dans les locaux de la cure.\"}>\n",
      "TokenList<La, réévaluation, de, cette, participation, était, à, l', ordre, de, le, jour, ., metadata={sent_id: \"annodis.er_00247\", text: \"La réévaluation de cette participation était à l'ordre du jour.\"}>\n",
      "TokenList<Le, comité, est, inchangé, :, metadata={sent_id: \"annodis.er_00256\", text: \"Le comité est inchangé :\"}>\n",
      "TokenList<vice-président, :, André, Ménétrez, ;, metadata={sent_id: \"annodis.er_00258\", text: \"vice-président : André Ménétrez ;\"}>\n",
      "TokenList<\", Nous, avions, bon, espoir, d', obtenir, d', elle, un, prêt-relais, pour, acheter, les, matières, premières, nécessaires, à, le, redémarrage, de, l', activité, ,, car, dans, l', usine, ,, les, machines, sont, arrêtées, depuis, le, 14, janvier, dernier, \", explique, le, directeur, d', EFI, Michel, Balandier, ., metadata={sent_id: \"annodis.er_00266\", text: \"\"Nous avions bon espoir d'obtenir d'elle un prêt-relais pour acheter les matières premières nécessaires au redémarrage de l'activité, car dans l'usine, les machines sont arrêtées depuis le 14 janvier dernier\" explique le directeur d'EFI Michel Balandier.\"}>\n",
      "TokenList<Et, comme, le, plan, de, résorption, de, les, dettes, prévoyait, un, autre, versement, de, près, de, deux, millions, d', euros, en, juin, prochain, ..., metadata={sent_id: \"annodis.er_00277\", text: \"Et comme le plan de résorption des dettes prévoyait un autre versement de près de deux millions d'euros en juin prochain...\"}>\n",
      "TokenList<Ils, pourront, découvrir, le, \", Planétarium, \", et, partir, à, la, rencontre, de, le, monde, merveilleux, de, les, étoiles, et, de, les, constellations, ., metadata={sent_id: \"annodis.er_00319\", text: \"Ils pourront découvrir le \"Planétarium\" et partir à la rencontre du monde merveilleux des étoiles et des constellations.\"}>\n",
      "TokenList<La, température, affichant, -6, °C, ,, il, emmenait, les, gamins, se, réchauffer, dans, les, camions, de, les, pompiers, ., metadata={sent_id: \"annodis.er_00327\", text: \"La température affichant -6°C, il emmenait les gamins se réchauffer dans les camions des pompiers.\"}>\n",
      "TokenList<Et, l', acte, malveillant, figure, parmi, les, hypothèses, ., metadata={sent_id: \"annodis.er_00331\", text: \"Et l'acte malveillant figure parmi les hypothèses.\"}>\n",
      "TokenList<Pour, éviter, une, reprise, de, le, feu, mais, aussi, pour, prévenir, des, actes, malveillants, \", constatait, Martial, Bourquin, ., metadata={sent_id: \"annodis.er_00336\", text: \"Pour éviter une reprise du feu mais aussi pour prévenir des actes malveillants\" constatait Martial Bourquin.\"}>\n",
      "TokenList<M., Rivasseau, a, réaffirmé, l', \", exigence, absolue, de, le, respect, de, les, droits, de, l', Homme, \", pour, Paris, alors, que, des, témoignages, de, réfugiés, font, état, d', exactions, ,, viols, ,, et, pillages, par, des, éléments, semble, -t-il, libériens, ., metadata={sent_id: \"annodis.er_00347\", text: \"M. Rivasseau a réaffirmé l'\"exigence absolue du respect des droits de l'Homme\" pour Paris alors que des témoignages de réfugiés font état d'exactions, viols, et pillages par des éléments semble-t-il libériens.\"}>\n"
     ]
    }
   ],
   "source": [
    "from conllu import parse_incr\n",
    "i = 0\n",
    "sent_dict = {}\n",
    "for tokenlist in parse_incr(open(\"../pstal-etu/sequoia/sequoia-ud.parseme.frsemcor.simple.small\", encoding='UTF-8')):\n",
    "    print(tokenlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba9a494-d158-49fa-9f6d-8b9f893c3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare the pred supertag morph with the gold\n",
    "\n",
    "import conllu\n",
    "from conllu import parse_incr\n",
    "\n",
    "gold_file = \"../pstal-etu/sequoia/sequoia-ud.parseme.frsemcor.simple.dev\"\n",
    "pred_file = \"./sequoia-ud.parseme.frsemcor.simple.pred\"\n",
    "\n",
    "gold_sents = parse_incr(open(gold_file, encoding=\"UTF-8\"))\n",
    "pred_sents = parse_incr(open(pred_file, encoding=\"UTF-8\"))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for (gold_sent, pred_sent) in zip(gold_sents, pred_sents):\n",
    "    for (gold_tok, pred_tok) in zip(gold_sent, pred_sent):\n",
    "        # print(f\"{gold_tok[\"id\"]}, {gold_tok[\"feats\"]} -------- {pred_tok[\"id\"]}, {pred_tok[\"feats\"]}\")\n",
    "        if str(gold_tok[\"feats\"]) == next(iter(pred_tok[\"feats\"].keys())):\n",
    "            correct += 1\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56237c00-6ee8-4286-9a1a-b5ec47a104fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.50731807874665"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(correct/total)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f6c92c-191b-4fe6-8b24-36947ff612e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'association a changé les décors et avec l'aide de plusieurs bénévoles, établi différents tableaux sur le thème de \"Cinq siècles d'activité économique de la région d'Ancerville\".\n"
     ]
    }
   ],
   "source": [
    "from conllu import parse_incr\n",
    "sent_list = []\n",
    "for sent in parse_incr(open(\"../pstal-etu/sequoia/sequoia-ud.parseme.frsemcor.simple.small\", encoding='UTF-8')):\n",
    "    sent_list.append(sent.metadata['text'])\n",
    "print(sent_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b7da33f-6ce5-47ee-8216-a6aa3bbcca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "chars = []\n",
    "in_enc = []\n",
    "ends = []\n",
    "vocab = [\"<pad>\", \"<unk>\"] + sorted(set(\"\".join(sent_list)))    \n",
    "vocab_int = {\"<pad>\": 0, \"<unk>\": 1, \"<esp>\": 2}\n",
    "for i, v in enumerate(vocab):\n",
    "    if v == ' ':\n",
    "        vocab[i] = '<esp>'\n",
    "        \n",
    "for s in sent_list:    \n",
    "    c = [\"<pad>\"] + [w if w != \" \" else \"<esp>\" for w in s]\n",
    "    chars.append(c)\n",
    "\n",
    "    char_to_int = [vocab.index(w) for w in c]\n",
    "    in_enc.append(char_to_int)\n",
    "    \n",
    "    end = []\n",
    "    for idx, w in enumerate(s): \n",
    "        if w == ' ':\n",
    "          end.append(idx-1)     \n",
    "    ends.append(end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2331b1c-1ac5-4d3c-b5c6-caa1406a18da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12,\n",
       "  14,\n",
       "  21,\n",
       "  25,\n",
       "  32,\n",
       "  35,\n",
       "  40,\n",
       "  47,\n",
       "  50,\n",
       "  60,\n",
       "  71,\n",
       "  78,\n",
       "  89,\n",
       "  98,\n",
       "  102,\n",
       "  105,\n",
       "  111,\n",
       "  114,\n",
       "  120,\n",
       "  128,\n",
       "  139,\n",
       "  150,\n",
       "  153,\n",
       "  156,\n",
       "  163],\n",
       " [8, 13, 17, 23, 28, 31, 37, 40, 46, 48],\n",
       " [4, 7, 20, 23, 32, 42, 51, 56, 60, 66],\n",
       " [2, 10, 13, 18, 25, 36, 39, 51, 55, 67, 71, 76, 82],\n",
       " [1, 9, 15, 18],\n",
       " [4,\n",
       "  7,\n",
       "  11,\n",
       "  14,\n",
       "  17,\n",
       "  23,\n",
       "  30,\n",
       "  36,\n",
       "  45,\n",
       "  53,\n",
       "  56,\n",
       "  64,\n",
       "  72,\n",
       "  75,\n",
       "  78,\n",
       "  92,\n",
       "  101,\n",
       "  107,\n",
       "  114,\n",
       "  116,\n",
       "  125,\n",
       "  128,\n",
       "  132,\n",
       "  139,\n",
       "  142,\n",
       "  150,\n",
       "  154,\n",
       "  165,\n",
       "  168,\n",
       "  189,\n",
       "  194,\n",
       "  200,\n",
       "  208,\n",
       "  211,\n",
       "  218,\n",
       "  225,\n",
       "  227],\n",
       " [3, 10, 16, 24, 33, 39, 49, 52, 64, 80, 83],\n",
       " [2,\n",
       "  9,\n",
       "  18,\n",
       "  21,\n",
       "  28,\n",
       "  31,\n",
       "  37,\n",
       "  40,\n",
       "  44,\n",
       "  54,\n",
       "  58,\n",
       "  67,\n",
       "  72,\n",
       "  75,\n",
       "  82,\n",
       "  86,\n",
       "  96,\n",
       "  107,\n",
       "  110,\n",
       "  116,\n",
       "  122],\n",
       " [1,\n",
       "  8,\n",
       "  18,\n",
       "  24,\n",
       "  29,\n",
       "  35,\n",
       "  38,\n",
       "  41,\n",
       "  46,\n",
       "  49,\n",
       "  63,\n",
       "  67,\n",
       "  71,\n",
       "  76,\n",
       "  92,\n",
       "  101,\n",
       "  104,\n",
       "  110,\n",
       "  112,\n",
       "  123,\n",
       "  126,\n",
       "  138,\n",
       "  147,\n",
       "  161,\n",
       "  165,\n",
       "  173,\n",
       "  176,\n",
       "  185,\n",
       "  188,\n",
       "  198,\n",
       "  211,\n",
       "  214,\n",
       "  220,\n",
       "  224,\n",
       "  227,\n",
       "  234,\n",
       "  237,\n",
       "  242,\n",
       "  248,\n",
       "  253,\n",
       "  256,\n",
       "  267,\n",
       "  276,\n",
       "  283,\n",
       "  285,\n",
       "  288,\n",
       "  295],\n",
       " [6, 13, 19, 23, 25, 38, 43, 46, 53, 56, 59, 67, 73, 79, 85, 93, 96, 101],\n",
       " [4,\n",
       "  9,\n",
       "  12,\n",
       "  20,\n",
       "  24,\n",
       "  26,\n",
       "  30,\n",
       "  42,\n",
       "  53,\n",
       "  58,\n",
       "  68,\n",
       "  72,\n",
       "  83,\n",
       "  86,\n",
       "  93,\n",
       "  96,\n",
       "  107,\n",
       "  113,\n",
       "  119,\n",
       "  123,\n",
       "  131,\n",
       "  134,\n",
       "  141,\n",
       "  148,\n",
       "  151,\n",
       "  159,\n",
       "  165,\n",
       "  173,\n",
       "  175,\n",
       "  181,\n",
       "  184,\n",
       "  189,\n",
       "  192,\n",
       "  201,\n",
       "  206,\n",
       "  210,\n",
       "  218,\n",
       "  221],\n",
       " [1, 6, 9, 18, 20, 33, 38, 41, 51, 54, 66, 73, 76, 85, 90, 94],\n",
       " [4, 8, 25, 28, 31, 37, 42, 54, 58, 64, 73, 76, 79, 87, 90, 95, 98, 101],\n",
       " [7, 13, 16, 24],\n",
       " [1,\n",
       "  6,\n",
       "  15,\n",
       "  22,\n",
       "  31,\n",
       "  35,\n",
       "  50,\n",
       "  54,\n",
       "  58,\n",
       "  70,\n",
       "  75,\n",
       "  79,\n",
       "  91,\n",
       "  96,\n",
       "  100,\n",
       "  111,\n",
       "  118,\n",
       "  121,\n",
       "  133,\n",
       "  136,\n",
       "  144,\n",
       "  147],\n",
       " [3,\n",
       "  8,\n",
       "  11,\n",
       "  19,\n",
       "  22,\n",
       "  26,\n",
       "  35,\n",
       "  38,\n",
       "  42,\n",
       "  47,\n",
       "  60,\n",
       "  63,\n",
       "  69,\n",
       "  73,\n",
       "  80,\n",
       "  85,\n",
       "  94,\n",
       "  97,\n",
       "  104,\n",
       "  111,\n",
       "  118,\n",
       "  121,\n",
       "  129,\n",
       "  139],\n",
       " [2,\n",
       "  5,\n",
       "  10,\n",
       "  14,\n",
       "  17,\n",
       "  23,\n",
       "  34,\n",
       "  39,\n",
       "  46,\n",
       "  49,\n",
       "  57,\n",
       "  60,\n",
       "  76,\n",
       "  85,\n",
       "  95,\n",
       "  104,\n",
       "  116,\n",
       "  120,\n",
       "  131,\n",
       "  136,\n",
       "  139],\n",
       " [4, 14, 21, 27, 31],\n",
       " [1,\n",
       "  12,\n",
       "  21,\n",
       "  25,\n",
       "  32,\n",
       "  42,\n",
       "  45,\n",
       "  49,\n",
       "  57,\n",
       "  60,\n",
       "  73,\n",
       "  81,\n",
       "  84,\n",
       "  88,\n",
       "  93,\n",
       "  106,\n",
       "  110,\n",
       "  115,\n",
       "  117,\n",
       "  122,\n",
       "  129,\n",
       "  133,\n",
       "  139,\n",
       "  142],\n",
       " [12, 24, 33, 38, 42, 49, 52, 55],\n",
       " [1, 14, 17, 23, 37, 43, 45, 53, 56],\n",
       " [1, 8, 12, 21],\n",
       " [13, 15, 21, 30],\n",
       " [4,\n",
       "  11,\n",
       "  15,\n",
       "  22,\n",
       "  32,\n",
       "  39,\n",
       "  42,\n",
       "  54,\n",
       "  59,\n",
       "  67,\n",
       "  71,\n",
       "  80,\n",
       "  90,\n",
       "  102,\n",
       "  105,\n",
       "  117,\n",
       "  120,\n",
       "  132,\n",
       "  136,\n",
       "  141,\n",
       "  150,\n",
       "  154,\n",
       "  163,\n",
       "  168,\n",
       "  177,\n",
       "  184,\n",
       "  187,\n",
       "  190,\n",
       "  198,\n",
       "  207,\n",
       "  216,\n",
       "  219,\n",
       "  229,\n",
       "  235,\n",
       "  242],\n",
       " [1,\n",
       "  7,\n",
       "  10,\n",
       "  15,\n",
       "  18,\n",
       "  29,\n",
       "  33,\n",
       "  40,\n",
       "  50,\n",
       "  53,\n",
       "  59,\n",
       "  69,\n",
       "  72,\n",
       "  77,\n",
       "  80,\n",
       "  85,\n",
       "  94,\n",
       "  102,\n",
       "  105,\n",
       "  110],\n",
       " [2, 11, 21, 24, 38, 41, 48, 50, 53, 63, 66, 72, 84, 88, 96, 99, 103],\n",
       " [1, 13, 23, 29, 32, 41, 45, 52, 55, 66, 71, 75, 83, 87],\n",
       " [1, 8, 20, 27, 33, 37],\n",
       " [3, 10, 14, 22, 25, 29, 34, 40, 45, 54, 58, 64, 78, 89, 97],\n",
       " [1,\n",
       "  11,\n",
       "  13,\n",
       "  23,\n",
       "  35,\n",
       "  43,\n",
       "  46,\n",
       "  54,\n",
       "  58,\n",
       "  65,\n",
       "  68,\n",
       "  77,\n",
       "  82,\n",
       "  88,\n",
       "  94,\n",
       "  98,\n",
       "  102,\n",
       "  114,\n",
       "  117,\n",
       "  126,\n",
       "  131,\n",
       "  136,\n",
       "  149,\n",
       "  156,\n",
       "  159,\n",
       "  168,\n",
       "  172,\n",
       "  176,\n",
       "  185,\n",
       "  197]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ad7750b-43ad-41ea-9c21-277eb2f496e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr\n",
    "feat_list = []\n",
    "for sent in parse_incr(open(\"../pstal-etu/sequoia/sequoia-ud.parseme.frsemcor.simple.small\", encoding='UTF-8')):\n",
    "    for tok in sent:\n",
    "        if tok[\"feats\"] is None:\n",
    "            feat_list.append(\"<N/A>\")\n",
    "        elif \"Number\" in tok[\"feats\"].keys():\n",
    "            feat_list.append(tok[\"feats\"][\"Number\"])\n",
    "        else:\n",
    "            feat_list.append(\"<N/A>\")\n",
    "\n",
    "number_feats = {w: i for i,w in enumerate(set(feat_list))}\n",
    "\n",
    "out_enc = [number_feats[w] for w in feat_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1597e543-570d-4217-a9c0-b093f1148cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24875\n"
     ]
    }
   ],
   "source": [
    "from conllu import parse_incr\n",
    "file_path = '../pstal-etu/sequoia/sequoia-ud.parseme.frsemcor.simple.full'\n",
    "\n",
    "counter = 0\n",
    "sents = parse_incr(open(file_path, encoding=\"UTF-8\"))\n",
    "for sent in sents:\n",
    "    for tok in sent:\n",
    "        if tok[\"feats\"] is None:\n",
    "            counter += 1\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b70d829-410d-438a-ad01-1aaf27902993",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diff_feats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m feats_values = {}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mdiff_feats\u001b[49m):\n\u001b[32m      3\u001b[39m     feats_values[df] = [\u001b[33m\"\u001b[39m\u001b[33m<N/A>\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m sents = parse_incr(\u001b[38;5;28mopen\u001b[39m(file_path, encoding=\u001b[33m\"\u001b[39m\u001b[33mUTF-8\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'diff_feats' is not defined"
     ]
    }
   ],
   "source": [
    "feats_values = {}\n",
    "for df in sorted(diff_feats):\n",
    "    feats_values[df] = [\"<N/A>\"]\n",
    "sents = parse_incr(open(file_path, encoding=\"UTF-8\"))\n",
    "for sent in sents:\n",
    "    for tok in sent:\n",
    "        if tok[\"feats\"] is not None:\n",
    "            for l in list(tok[\"feats\"].keys()):\n",
    "                if tok[\"feats\"][l] not in feats_values[l]:\n",
    "                    feats_values[l].append(tok[\"feats\"][l])\n",
    "\n",
    "print(feats_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55d147-9f78-4839-bfb0-cec0e3d770b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in feats_values.items():\n",
    "    feats_values[k] = [{w: i} for i,w in enumerate(feats_values[k])] \n",
    "print(feats_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e7e4c-1f29-4690-be01-f578f56cad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_counter = 0\n",
    "tok_counter = 0\n",
    "sents = parse_incr(open(file_path, encoding=\"UTF-8\"))\n",
    "for sent in sents:\n",
    "    for tok in sent:\n",
    "        tok_counter += 1\n",
    "        if tok[\"feats\"] is not None:\n",
    "            feat_counter += len(list(tok[\"feats\"].keys()))\n",
    "\n",
    "print(f\"{feat_counter/tok_counter:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea7fb9-2adc-4c71-a556-fd5b9ad91362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous avons deux types d’entrées : établissez un nombre maximal de caractères (p.ex. max_c=200) pour in_enc, et un nombre maximal de mots\n",
    "# (p.ex. max_w=20) pour ends et out_enc. Les phrases dépassant ces seuils seront coupées (crop). Pour le vecteur\n",
    "# ends, le padding est constitué de zéros, comme pour les deux autres. Nottez que Util.dataloader peut prendre\n",
    "# plusieurs tenseurs en entrée (in_enc et ends), il n’est pas nécessaire d’avoir des entrées et sorties uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c05bd1f-da97-43a4-94d1-15fc7576a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils_morph import load_chars, load_feats, build_feats_dict, pad_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fb37bf6-7e8d-4ff7-842a-4edd761c2a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', 'G', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g']\n",
      "[0, 37, 80, 79, 64, 73, 61, 64, 77, 66]\n",
      "[[]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "def unique_feats(in_file: str):\n",
    "    diff_feats = []\n",
    "    file_buffer = open(in_file, encoding=\"UTF-8\")\n",
    "    sents = parse_incr(file_buffer)\n",
    "    for sent in sents:\n",
    "        for tok in sent:\n",
    "            if tok[\"feats\"] is not None:\n",
    "                for l in list(tok[\"feats\"].keys()):\n",
    "                    diff_feats.append(l)\n",
    "    file_buffer.close()\n",
    "    return list(set(diff_feats))\n",
    "\n",
    "chars, in_enc, ends = load_chars(file_path, True)\n",
    "feat = \"Number\"\n",
    "diff_feat = unique_feats(file_path)\n",
    "feat_dict = build_feats_dict(file_path, diff_feat)\n",
    "out_enc = load_feats(file_path, feat, feat_dict)\n",
    "\n",
    "print(chars[0])\n",
    "print(in_enc[0])\n",
    "print(ends)\n",
    "print(out_enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29c641ee-c579-4d1b-a87e-f5286c0ddfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(out_enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d88984bb-ae93-41e1-9455-b6907e752344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils_morph import pad_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49b2f8b4-3fe3-4db8-b169-7cc050926094",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_c = 300\n",
    "max_w = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ee4e191-70f0-4657-a979-27514e9299e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (1983890125.py, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[31m    \u001b[39m\n    ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def build_loader(in_enc, ends, out_enc, max_c, max_w, batch_size, batch_mode=True):\n",
    "    if batch_size:\n",
    "        in_enc_tensor = pad_tensor(in_enc, max_c)\n",
    "        ends_tensor = pad_tensor(ends, max_w)\n",
    "        out_enc_tensor = pad_tensor(out_enc, max_w)\n",
    "        dataset = TensorDataset(in_enc_tensor, ends_tensor, out_enc_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=train_mode)\n",
    "        return dataloader, in_enc, ends, out_enc\n",
    "    else:\n",
    "        return in_enc, ends_out_enc\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51949068-cf87-4bae-9026-6c498f9730c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feats(in_file: str, feat: str, feat_dict: dict):\n",
    "    feat_list = []\n",
    "    file_buffer = open(in_file, encoding=\"UTF-8\")\n",
    "    sents = parse_incr(file_buffer)\n",
    "    for sent in sents:\n",
    "        feat_sent = []\n",
    "        for tok in sent:    \n",
    "            if tok[\"feats\"] is None:\n",
    "                feat_sent.append(\"<N/A>\")\n",
    "            elif feat in tok[\"feats\"].keys():\n",
    "                feat_sent.append(tok[\"feats\"][feat])\n",
    "            else:\n",
    "                feat_sent.append(\"<N/A>\")\n",
    "        feat_list.append(feat_sent)\n",
    "\n",
    "    out_enc = []\n",
    "    for fs in feat_list:\n",
    "        out_enc.append([feat_dict[feat][w] for w in fs])\n",
    "\n",
    "    file_buffer.close()\n",
    "\n",
    "    \n",
    "    return out_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee6db9c4-73c4-4121-b2ee-43f648a2a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils_morph import build_feats_dict, unique_feats\n",
    "diff_feat = unique_feats(file_path)\n",
    "out_enc = load_feats(file_path, \"Number\", build_feats_dict(file_path, diff_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85cfa249-7723-440b-aea9-c1203b3c1fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_enc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f40086-3d09-44e2-9c40-086507733ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
